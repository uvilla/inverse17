{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Denoising: Tikhonov and Total Variation Regularization\n",
    "\n",
    "The problem of removing noise from an image without blurring\n",
    "sharp edges can be formulated as an infinite-dimensional minimization\n",
    "problem. Given a possibly noisy image $u_0(x,y)$ defined within a\n",
    "square domain $\\Omega$, we would like to\n",
    "find the image $u(x,y)$ that is closest in the $L_2$ sense, i.e. we\n",
    "want to minimize \n",
    "\n",
    "$$ \\mathcal{F}_{LS} := \\frac{1}{2}\\int_\\Omega (u - u_0)^2 \\; d\\boldsymbol{x}, $$\n",
    "\n",
    "\n",
    "while also removing noise, which is assumed to comprise very ``rough''\n",
    "components of the image. This latter goal can be incorporated as an\n",
    "additional term in the objective, in the form of a penalty, i.e., \n",
    "\n",
    "$$ \\mathcal{R}_{TN} := \\! \\frac{\\alpha}{2}\\int_\\Omega \\nabla u\n",
    "\\cdot \\! \\nabla u \\; d\\boldsymbol{x}, $$\n",
    "where $\\alpha$ acts as a ``diffusion'' coefficient that controls\n",
    "how strongly we impose the penalty, i.e.\\ how much smoothing\n",
    "occurs. Unfortunately, if there are sharp edges in the image, this\n",
    "so-called {\\em Tikhonov (TN) regularization} will blur them. Instead,\n",
    "in these cases we prefer the so-called {\\em total variation (TV)\n",
    "regularization},\n",
    "\n",
    "$$ \\mathcal{R}_{TV} := \\! \\alpha\\int_\\Omega (\\nabla u \\cdot \\! \\nabla\n",
    "u)^{\\frac{1}{2}} \\; d\\boldsymbol{x} $$\n",
    "\n",
    "where (we will see that) taking the square root is the key to\n",
    "preserving edges. Since \n",
    "$\\mathcal{R}_{TV}$ is not differentiable when $\\nabla u =\n",
    "\\boldsymbol{0}$, it is usually modified to include a positive parameter\n",
    "$\\varepsilon$ as follows:\n",
    "\n",
    "$$ \\mathcal{R}^{\\varepsilon}_{TV} := \\!  \\alpha \\int_\\Omega (\\nabla u \\cdot\n",
    "\\! \\nabla u + \\varepsilon)^{\\frac{1}{2}} \\; d\\boldsymbol{x}. $$\n",
    "We wish to study the performance of the two denoising functionals\n",
    "$\\mathcal{F}_{TN}$ and $\\mathcal{F}^{\\varepsilon}_{TV}$, where\n",
    "\n",
    "$$ \\mathcal{F}_{TN}  := \\mathcal{F}_{LS} + \\mathcal{R}_{TN} $$\n",
    "and\n",
    "\n",
    "$$ \\mathcal{F}^{\\varepsilon}_{TV}  := \\mathcal{F}_{LS} + \\mathcal{R}^{\\varepsilon}_{TV}. $$\n",
    "\n",
    "We will prescribe the homogeneous Neumann condition \n",
    "$\\nabla u \\cdot \\boldsymbol{n}=0$ on the four sides of the square,\n",
    "which amounts to assuming that the image intensity does not change\n",
    "normal to the boundary of the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "- We generate a rectangular mesh of size `Lx` by `Ly`.\n",
    "\n",
    "- We define a linear finite element space.\n",
    "\n",
    "- We generate two finite element functions `u_true` and `u_0` that represent the true image and the noisy image respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from dolfin import *\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "from unconstrainedMinimization import InexactNewtonCG\n",
    "\n",
    "logging.getLogger('FFC').setLevel(logging.WARNING)\n",
    "logging.getLogger('UFL').setLevel(logging.WARNING)\n",
    "set_log_active(False)\n",
    "\n",
    "data = np.loadtxt('image.dat', delimiter=',')\n",
    "np.random.seed(seed=1)\n",
    "noise_std_dev = .3\n",
    "noise = noise_std_dev*np.random.randn(data.shape[0], data.shape[1])\n",
    "Lx = float(data.shape[1])/float(data.shape[0])\n",
    "Ly = 1.\n",
    "\n",
    "class Image(Expression):\n",
    "    def __init__(self, Lx, Ly, data, **kwargs):\n",
    "        self.data = data\n",
    "        self.hx = Lx/float(data.shape[1]-1)\n",
    "        self.hy = Ly/float(data.shape[0]-1)\n",
    "        \n",
    "    def eval(self, values, x):\n",
    "        j = int(math.floor(x[0]/self.hx))\n",
    "        i = int(math.floor(x[1]/self.hy))\n",
    "        values[0] = self.data[i,j]\n",
    "        \n",
    "mesh = RectangleMesh(Point(0,0),Point(Lx,Ly),200, 100)\n",
    "V = FunctionSpace(mesh, \"Lagrange\",1)\n",
    "trueImage = Image(Lx,Ly,data,degree = 1)\n",
    "noisyImage = Image(Lx,Ly,data+noise, degree = 1)\n",
    "u_true  = interpolate(trueImage, V)\n",
    "u_0 = interpolate(noisyImage, V)\n",
    "\n",
    "\n",
    "plt.figure(figsize=[12,24])\n",
    "plt.subplot(1,2,1)\n",
    "plot(u_true, title=\"True Image\")\n",
    "plt.subplot(1,2,2)\n",
    "plot(u_0, title=\"Noisy Image\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The misfit functional and the true error functional\n",
    "\n",
    "Here we describe the variational forms for the $L_2$ misfit functional\n",
    "$$ \\mathcal{F}_{LS} := \\frac{1}{2}\\int_\\Omega (u - u_0)^2 \\; d\\boldsymbol{x}, $$\n",
    "and its first and second variations.\n",
    "\n",
    "Since we also know the true image `u_true` (*this will not be the case for real applications*)\n",
    "we can also write the true $L_2$ error functional \n",
    "$$ \\mathcal{F}_{\\rm true} := \\frac{1}{2}\\int_\\Omega (u - u_{\\rm true})^2 \\; d\\boldsymbol{x}. $$\n",
    "\n",
    "Finally, we define some values of $\\alpha$ ($\\alpha = 1, 10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}$) for the choice of the regularization paramenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u = Function(V)\n",
    "u_hat = TestFunction(V)\n",
    "u_tilde = TrialFunction(V)\n",
    "\n",
    "F_ls = Constant(.5)*inner(u - u_0, u - u_0)*dx\n",
    "grad_ls = inner(u - u_0, u_hat)*dx\n",
    "H_ls = inner(u_tilde, u_hat)*dx\n",
    "\n",
    "trueError = inner(u - u_true, u - u_true)*dx\n",
    "\n",
    "n_alphas = 5\n",
    "alphas = np.power(10., -np.arange(n_alphas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tikhonov regularization\n",
    "\n",
    "The function `TNsolution` computes the solution of the denoising inverse problem using Tikhonov regularization for a given amount a regularization $\\alpha$.\n",
    "\n",
    "More specifically it minimizes the functional\n",
    "$$ \\mathcal{F}_{TN}(u) = \\frac{1}{2}\\int_\\Omega (u - u_0)^2 \\; d\\boldsymbol{x} + \\frac{\\alpha}{2}\\int_\\Omega \\nabla u \\cdot \\nabla u d\\boldsymbol{x}. $$\n",
    "\n",
    "The first variation of $\\mathcal{F}_{TN}$ reads\n",
    "$$ \\delta_u \\mathcal{F}_{TN}(u, \\hat{u}) = \\int_\\Omega (u - u_0) \\hat{u} \\; d\\boldsymbol{x} + \\alpha \\int_\\Omega \\nabla u \\cdot \\nabla \\hat{u}  d\\boldsymbol{x}, $$\n",
    "and the second variation is\n",
    "$$ \\delta_u^2 \\mathcal{F}_{TN}(u, \\hat{u}, \\tilde{u}) = \\int_\\Omega \\tilde{u}\\, \\hat{u} \\; d\\boldsymbol{x} + \\alpha \\int_\\Omega \\nabla \\tilde{u} \\cdot \\nabla \\hat{u}\\, d\\boldsymbol{x}. $$\n",
    "\n",
    "The best reconstruction of the original image is obtained for $\\alpha = 10^{-3}$, however we notice that the sharp edges of the image are lost in the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TNsolution(alpha):\n",
    "    R_tn = Constant(.5)*inner(nabla_grad(u), nabla_grad(u))*dx\n",
    "    grad_tn = inner(nabla_grad(u), nabla_grad(u_hat))*dx\n",
    "    H_tn = inner(nabla_grad(u_tilde), nabla_grad(u_hat))*dx\n",
    "    F = F_ls + alpha*R_tn\n",
    "    grad = grad_ls + alpha*grad_tn\n",
    "    H = H_ls + alpha*H_tn\n",
    "    solve(H==-grad, u)\n",
    "    print \"{0:15e} {1:15e} {2:15e} {3:15e} {4:15e}\".format(\n",
    "           alphas[i], assemble(F), assemble(F_ls), assemble(alpha*R_tn), assemble(trueError))\n",
    "    plt.figure()\n",
    "    plot(u)\n",
    "    plt.show()\n",
    "\n",
    "print \"{0:15} {1:15} {2:15} {3:15} {4:15}\".format(\"alpha\", \"F\", \"F_ls\", \"alpha*R_tn\", \"True Error\")\n",
    "for i in range(n_alphas):\n",
    "    TNsolution(Constant(alphas[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Total Variation regularization\n",
    "\n",
    "The function `TVsolution` computes the solution of the denoising inverse problem using Total Variation regularization for a given amount a regularization $\\alpha$ and perturbation $\\varepsilon$.\n",
    "\n",
    "More specifically it minimizes the functional\n",
    "$$ \\mathcal{F}_{TV}(u) = \\frac{1}{2}\\int_\\Omega (u - u_0)^2 \\; d\\boldsymbol{x} + \\frac{\\alpha}{2}\\int_\\Omega \\left( \\nabla u \\cdot \\nabla u + \\varepsilon \\right)^{\\frac{1}{2}} d\\boldsymbol{x}. $$\n",
    "\n",
    "The first variation of $\\mathcal{F}_{TV}$ reads\n",
    "$$ \\delta_u \\mathcal{F}_{TV}(u, \\hat{u}) = \\int_\\Omega (u - u_0)\\hat{u}  \\; d\\boldsymbol{x} + \\alpha \\int_\\Omega \\frac{1}{\\left( \\nabla u \\cdot \\nabla u + \\varepsilon \\right)^{\\frac{1}{2}}}\\nabla u \\cdot \\nabla \\hat{u}  d\\boldsymbol{x}, $$\n",
    "and the second variation is\n",
    "$$ \\delta_u^2 \\mathcal{F}_{TV}(u, \\hat{u}, \\tilde{u}) = \\int_\\Omega \\tilde{u} \\hat{u} \\; d\\boldsymbol{x} + \\alpha \\int_\\Omega \\frac{1}{\\left( \\nabla u \\cdot \\nabla u + \\varepsilon \\right)^{\\frac{1}{2}}} \\left[ \\left( I - \\frac{\\nabla u \\otimes \\nabla u}{\\nabla u \\cdot \\nabla u + \\varepsilon}\\right) \\nabla \\tilde{u}\\right] \\cdot \\nabla \\hat{u} d\\boldsymbol{x}. $$\n",
    "\n",
    "The highly nonlinear term $\\left( I - \\frac{\\nabla u \\otimes \\nabla u}{\\nabla u \\cdot \\nabla u + \\varepsilon}\\right) $ in the second variation poses a substantial challange for the convergence of the Newton's method. In fact, the converge radius of the Newtos's method is extremely small.\n",
    "For this reason in the following we will replace the second variation with the variational form\n",
    "$$ H_{\\rm approx} = \\int_\\Omega \\tilde{u}\\,\\hat{u} \\; d\\boldsymbol{x} + \\alpha \\int_\\Omega \\frac{1}{\\left( \\nabla u \\cdot \\nabla u + \\varepsilon \\right)^{\\frac{1}{2}}} \\tilde{u} \\cdot \\nabla \\hat{u} d\\boldsymbol{x}. $$\n",
    "\n",
    "The resulting method will exhibit only a first order convergence rate but it will be more robust for small values of $\\varepsilon$.\n",
    "\n",
    "The best reconstruction of the original image is obtained for $\\alpha = 10^{-2}$. We also notice that Total Variation does a much better job that Tikhonov regularization in preserving the sharp edges of the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TVsolution(eps, alpha):\n",
    "    def TV(u, eps):\n",
    "        return sqrt( inner(nabla_grad(u), nabla_grad(u)) + eps)\n",
    "    \n",
    "    def scaled_grad(u, eps):\n",
    "        return nabla_grad(u)/TV(u,eps)\n",
    "    \n",
    "    R_tv = TV(u, eps)*dx\n",
    "    F = F_ls + alpha*R_tv\n",
    "    grad_tv = Constant(1.)/TV(u, eps)*inner(nabla_grad(u), nabla_grad(u_hat))*dx\n",
    "    grad = grad_ls + alpha*grad_tv\n",
    "    H_tv = Constant(1.)/TV(u, eps)*inner(nabla_grad(u_tilde), nabla_grad(u_hat))*dx #+ \\\n",
    "    #       Constant(1.)/TV(u, eps)*inner(outer(scaled_grad(u,eps), scaled_grad(u,eps) )*nabla_grad(u_tilde), nabla_grad(u_hat))*dx \n",
    "    H = H_ls + alpha*H_tv\n",
    "    solver = InexactNewtonCG()\n",
    "    solver.parameters[\"rel_tolerance\"] = 1e-5\n",
    "    solver.parameters[\"abs_tolerance\"] = 1e-6\n",
    "    solver.parameters[\"gdu_tolerance\"] = 1e-18\n",
    "    solver.parameters[\"max_iter\"] = 1000\n",
    "    solver.parameters[\"c_armijo\"] = 1e-5\n",
    "    solver.parameters[\"print_level\"] = -1\n",
    "    solver.parameters[\"max_backtracking_iter\"] = 10\n",
    "    solver.solve(F, u, grad, H)\n",
    "\n",
    "    print \"{0:15e} {1:5} {2:4d} {3:15e} {4:15e} {5:15e} {6:15e}\".format(\n",
    "           alphas[i], solver.converged, solver.it, assemble(F), assemble(F_ls), assemble(alpha*R_tv), assemble(trueError))\n",
    "    \n",
    "    plt.figure()\n",
    "    plot(u)\n",
    "    plt.show()\n",
    "    \n",
    "print \"{0:15} {1:5} {2:4} {3:15} {4:15} {5:15} {6:15}\".format(\"alpha\", \"converged\", \"nit\", \"F\", \"F_ls\", \"alpha*R_tn\", \"True Error\")\n",
    "\n",
    "eps = Constant(1e-4)\n",
    "for i in range(n_alphas):\n",
    "    TVsolution(eps, Constant(alphas[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2016, The University of Texas at Austin & University of California, Merced.\n",
    "\n",
    "All Rights reserved.\n",
    "See file COPYRIGHT for details.\n",
    "\n",
    "This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io.\n",
    "\n",
    "hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
